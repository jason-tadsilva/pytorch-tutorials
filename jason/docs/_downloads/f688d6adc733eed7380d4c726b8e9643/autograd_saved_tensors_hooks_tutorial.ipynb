{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n# https://pytorch.org/tutorials/beginner/colab\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Hooks for autograd saved tensors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PyTorch typically computes gradients using backpropagation. However,\ncertain operations require intermediary results to be saved in order to\nperform backpropagation. This tutorial walks through how these tensors\nare saved/retrieved and how you can define hooks to control the\npacking/unpacking process.\n\nThis tutorial assumes you are familiar with how backpropagation works in\ntheory. If not, read [this](https://colab.research.google.com/drive/1aWNdmYt7RcHMbUk-Xz2Cv5-cGFSWPXe0#scrollTo=AHcEJ6nXUb7W) first.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Saved tensors\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training a model usually consumes more memory than running it for\ninference. Broadly speaking, one can say that it is because \u201cPyTorch\nneeds to save the computation graph, which is needed to call\n``backward``\u201d, hence the additional memory usage. One goal of this\ntutorial is to finetune this understanding.\n\nIn fact, the graph in itself sometimes does not consume much more memory\nas it never copies any tensors. However, the graph can keep *references*\nto tensors that would otherwise have gone out of scope: those are\nreferred to as **saved tensors**.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Why does training a model (typically) requires more memory than evaluating it?\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We start with a simple example: $y = a \\cdot b$ , for which\nwe know the gradients of $y$ with respect to $a$ and\n$b$:\n\n\\begin{align}\\frac{\\partial y}{\\partial a} = b\\end{align}\n\n\\begin{align}\\frac{\\partial y}{\\partial b} = a\\end{align}\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\n\na = torch.randn(5, requires_grad=True)\nb = torch.ones(5, requires_grad=True)\ny = a * b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using a torchviz, we can visualize the computation graph\n\n .. figure:: https://user-images.githubusercontent.com/8019486/130124513-72e016a3-c36f-42b9-88e2-53baf3e016c5.png\n   :width: 300\n   :align: center\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example, PyTorch saves intermediary values $a$ and\n$b$ in order to compute the gradient during the backward.\n\n .. figure:: https://user-images.githubusercontent.com/8019486/130124538-3da50977-6f0b-46d0-8909-5456ade9b598.png\n   :width: 300\n   :align: center\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Those intermediary values (in orange above) can be accessed (for\ndebugging purposes) by looking for attributes of the ``grad_fn`` of\n``y`` which start with the prefix ``_saved``:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(y.grad_fn._saved_self)\nprint(y.grad_fn._saved_other)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As the computation graph grows in depth, it will store more *saved\ntensors*. Meanwhile, those tensors would have gone out of scope if not\nfor the graph.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def f(x):\n    return x * x\n\nx = torch.randn(5, requires_grad=True)\ny = f(f(f(x)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. figure:: https://user-images.githubusercontent.com/8019486/130124570-f1074098-1bb3-459e-bf5a-03bf6f65b403.png\n  :width: 500\n  :align: center\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the example above, executing without grad would only have kept ``x``\nand ``y`` in the scope, But the graph additionally stores ``f(x)`` and\n``f(f(x))``. Hence, running a forward pass during training will be more\ncostly in memory usage than during evaluation (more precisely, when\nautograd is not required).\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The concept of packing / unpacking\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Going back to the first example: ``y.grad_fn._saved_self`` and\n``y.grad_fn._saved_other`` point to the original tensor object,\nrespectively ``a`` and ``b``.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "a = torch.randn(5, requires_grad=True)\nb = torch.ones(5, requires_grad=True)\ny = a * b\n\nprint(y.grad_fn._saved_self is a)   # True\nprint(y.grad_fn._saved_other is b)  # True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "However, that may not always be the case.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "a = torch.randn(5, requires_grad=True)\ny = torch.exp(a)\nprint(y.grad_fn._saved_result.equal(y))  # True\nprint(y.grad_fn._saved_result is y)      # False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Under the hood, PyTorch has **packed** and **unpacked** the tensor\n``y`` to prevent reference cycles.\n\nAs a rule of thumb, you should *not* rely on the fact that accessing\nthe tensor saved for backward will yield the same tensor object as the\noriginal tensor. They will however share the same *storage*.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Saved tensors hooks\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PyTorch provides an API to control how saved tensors should be packed /\nunpacked.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def pack_hook(x):\n    print(\"Packing\", x)\n    return x\n\ndef unpack_hook(x):\n    print(\"Unpacking\", x)\n    return x\na = torch.ones(5, requires_grad=True)\nb = torch.ones(5, requires_grad=True) * 2\n\nwith torch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook):\n    y = a * b\n\ny.sum().backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The ``pack_hook`` function will be called every time an operation saves\na tensor for backward.\nThe output of ``pack_hook`` is then stored in the computation graph\ninstead of the original tensor.\nThe ``unpack_hook`` uses that return value to compute a new tensor,\nwhich is the one actually used during the backward pass.\nIn general, you want ``unpack_hook(pack_hook(t))`` to be equal to\n``t``.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = torch.randn(5, requires_grad=True)\nwith torch.autograd.graph.saved_tensors_hooks(lambda x: x * 4, lambda x: x / 4):\n    y = torch.pow(x, 2)\ny.sum().backward()\nassert(x.grad.equal(2 * x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "One thing to note is that the output of ``pack_hook`` can be *any Python\nobject*, as long as ``unpack_hook`` can derive a tensor with the correct\nvalue from it.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Some unconventional examples\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, some silly examples to illustrate what is possible but you\nprobably don\u2019t ever want to do it.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Returning an ``int``\n\nReturning the index of a Python list\nRelatively harmless but with debatable usefulness\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "storage = []\n\ndef pack(x):\n    storage.append(x)\n    return len(storage) - 1\n\ndef unpack(x):\n    return storage[x]\n\nx = torch.randn(5, requires_grad=True)\nwith torch.autograd.graph.saved_tensors_hooks(pack, unpack):\n    y = x * x\ny.sum().backward()\n\nassert(x.grad.equal(2 * x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Returning a tuple\n\nReturning some tensor and a function how to unpack it\nQuite unlikely to be useful in its current form\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def pack(x):\n    delta = torch.randn(*x.size())\n    return x - delta, lambda x: x + delta\n\ndef unpack(packed):\n    x, f = packed\n    return f(x)\n\n\nx = torch.randn(5, requires_grad=True)\nwith torch.autograd.graph.saved_tensors_hooks(pack, unpack):\n    y = x * x\ny.sum().backward()\n\nassert(torch.allclose(x.grad, 2 * x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Returning a ``str``\n\nReturning the ``__repr__ of`` the tensor\nProbably never do this\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = torch.randn(5, requires_grad=True)\nwith torch.autograd.graph.saved_tensors_hooks(lambda x: repr(x), lambda x: eval(\"torch.\" + x)):\n    y = x * x\ny.sum().backward()\nassert(torch.all(x.grad - 2 * x <= 1e-4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Although those examples will not be useful in practice, they\nillustrate that the output of ``pack_hook`` can really be any Python\nobject as long as it contains enough information to retrieve the\ncontent of the original tensor.\nIn the next sections, we focus on more useful applications.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Saving tensors to CPU\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Very often, the tensors involved in the computation graph live on GPU.\nKeeping a reference to those tensors in the graph is what causes most\nmodels to run out of GPU memory during training while they would have\ndone fine during evaluation.\n\nHooks provide a very simple way to implement that.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def pack_hook(x):\n    return (x.device, x.cpu())\n\ndef unpack_hook(packed):\n    device, tensor = packed\n    return tensor.to(device)\n\nx = torch.randn(5, requires_grad=True)\nwith torch.autograd.graph.saved_tensors_hooks(pack, unpack):\n    y = x * x\ny.sum().backward()\n\ntorch.allclose(x.grad, (2 * x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In fact, PyTorch provides an API to conveniently use those hooks (as\nwell as the ability to use pinned memory).\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.w = nn.Parameter(torch.randn(5))\n\n    def forward(self, x):\n        with torch.autograd.graph.save_on_cpu(pin_memory=True):\n            # some computation\n            return self.w * x\n\nx = torch.randn(5)\nmodel = Model()\nloss = model(x).sum()\nloss.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In practice, on a A100 GPU, for a ResNet-152 with batch size 256, this\ncorresponds to a GPU memory usage reduction from 48GB to 5GB, at the\ncost of a 6x slowdown.\n\nOf course, you can modulate the tradeoff by only saving to CPU certain\nparts of the network.\n\nFor instance, you could define a special ``nn.Module`` that wraps any\nmodule and saves its tensors to CPU.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class SaveToCpu(nn.Module):\n    def __init__(self, module):\n        super().__init__()\n        self.module = module\n\n    def forward(self, *args, **kwargs):\n        with torch.autograd.graph.save_on_cpu(pin_memory=True):\n            return self.module(*args, **kwargs)\n\nmodel = nn.Sequential(\n    nn.Linear(10, 100),\n    SaveToCpu(nn.Linear(100, 100)),\n    nn.Linear(100, 10),\n)\n\nx = torch.randn(10)\nloss = model(x).sum()\nloss.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Saving tensors to disk\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similarly, you may want to save those tensors to disk. Again, this is\nachievable with those hooks.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A naive version would look like this.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Naive version - HINT: Don't do this\n\nimport uuid\ntmp_dir = \"temp\"\n\ndef pack_hook(tensor):\n    name = os.path.join(tmp_dir, str(uuid.uuid4()))\n    torch.save(tensor, name)\n    return name\n\ndef unpack_hook(name):\n    return torch.load(name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The reason the above code is bad is that we are leaking files on the\ndisk and they are never cleared. Fixing this is not as trivial as it\nseems.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Incorrect version - HINT: Don't do this\n\nimport uuid\nimport os\nimport tempfile\ntmp_dir_obj = tempfile.TemporaryDirectory()\ntmp_dir = tmp_dir_obj.name\n\ndef pack_hook(tensor):\n    name = os.path.join(tmp_dir, str(uuid.uuid4()))\n    torch.save(tensor, name)\n    return name\n\ndef unpack_hook(name):\n    tensor = torch.load(name)\n    os.remove(name)\n    return tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The reason the above code doesn\u2019t work is that ``unpack_hook`` can be\ncalled multiple times. If we delete the file during unpacking the first\ntime, it will not be available when the saved tensor is accessed a\nsecond time, which will raise an error.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = torch.ones(5, requires_grad=True)\nwith torch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook):\n    y = x.pow(2)\nprint(y.grad_fn._saved_self)\ntry:\n    print(y.grad_fn._saved_self)\n    print(\"Double access succeeded!\")\nexcept:\n    print(\"Double access failed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To fix this, we can write a version of those hooks that takes advantage\nof the fact that PyTorch automatically releases (deletes) the saved data\nwhen it is no longer needed.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class SelfDeletingTempFile():\n    def __init__(self):\n        self.name = os.path.join(tmp_dir, str(uuid.uuid4()))\n\n    def __del__(self):\n        os.remove(self.name)\n\ndef pack_hook(tensor):\n    temp_file = SelfDeletingTempFile()\n    torch.save(tensor, temp_file.name)\n    return temp_file\n\ndef unpack_hook(temp_file):\n    return torch.load(temp_file.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When we call ``backward``, the output of ``pack_hook`` will be deleted,\nwhich causes the file to be removed, so we\u2019re no longer leaking the\nfiles.\n\nThis can then be used in your model, in the following way:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Only save on disk tensors that have size >= 1000\nSAVE_ON_DISK_THRESHOLD = 1000\n\ndef pack_hook(x):\n    if x.numel() < SAVE_ON_DISK_THRESHOLD:\n        return x\n    temp_file = SelfDeletingTempFile()\n    torch.save(tensor, temp_file.name)\n    return temp_file\n\ndef unpack_hook(tensor_or_sctf):\n    if isinstance(tensor_or_sctf, torch.Tensor):\n        return tensor_or_sctf\n    return torch.load(tensor_or_sctf.name)\n\nclass SaveToDisk(nn.Module):\n    def __init__(self, module):\n        super().__init__()\n        self.module = module\n\n    def forward(self, *args, **kwargs):\n        with torch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook):\n            return self.module(*args, **kwargs)\n\nnet = nn.DataParallel(SaveToDisk(Model()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this last example, we also demonstrate how to filter which tensors\nshould be saved (here, those whose number of elements is greater than\n1000) and how to combine this feature with ``nn.DataParallel``.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you\u2019ve made it this far, congratulations! You now know how to use\nsaved tensor hooks and how they can be useful in a few scenarios to\ntradeoff memory for compute.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}