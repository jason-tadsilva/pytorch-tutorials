{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n# https://pytorch.org/tutorials/beginner/colab\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# (Beta) Implementing High-Performance Transformers with Scaled Dot Product Attention (SDPA)\n\n\n**Author:** [Driss Guessous](https://github.com/drisspg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n\nIn this tutorial, we want to highlight a new ``torch.nn.functional`` function\nthat can be helpful for implementing transformer architectures. The\nfunction is named ``torch.nn.functional.scaled_dot_product_attention``.\nFor detailed description of the function, see the [PyTorch documentation](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html#torch.nn.functional.scaled_dot_product_attention)_.\nThis function has already been incorporated into ``torch.nn.MultiheadAttention`` and ``torch.nn.TransformerEncoderLayer``.\n\n## Overview\nAt a high level, this PyTorch function calculates the\nscaled dot product attention (SDPA) between query, key, and value according to\nthe definition found in the paper [Attention is all you\nneed](https://arxiv.org/abs/1706.03762)_. While this function can\nbe written in PyTorch using existing functions, a fused implementation can provide\nlarge performance benefits over a naive implementation.\n\n## Fused implementations\n\nFor CUDA tensor inputs, the function will dispatch into one of the following\nimplementations:\n\n* [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)_\n* [Memory-Efficient Attention](https://github.com/facebookresearch/xformers)_\n* A PyTorch implementation defined in C++\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>This tutorial requires PyTorch 2.0.0 or later.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Example Usage:\nquery, key, value = torch.randn(2, 3, 8, device=device), torch.randn(2, 3, 8, device=device), torch.randn(2, 3, 8, device=device)\nF.scaled_dot_product_attention(query, key, value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Explicit Dispatcher Control\n\nWhile the function will implicitly dispatch to one of the three\nimplementations, the user can also explicitly control the dispatch via\nthe use of a context manager. This context manager allows users to\nexplicitly disable certain implementations. If a user wants to ensure\nthe function is indeed using the fastest implementation for their\nspecific inputs, the context manager can be used to sweep through\nmeasuring performance.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Lets define a helpful benchmarking function:\nimport torch.utils.benchmark as benchmark\ndef benchmark_torch_function_in_microseconds(f, *args, **kwargs):\n    t0 = benchmark.Timer(\n        stmt=\"f(*args, **kwargs)\", globals={\"args\": args, \"kwargs\": kwargs, \"f\": f}\n    )\n    return t0.blocked_autorange().mean * 1e6\n\n# Lets define the hyper-parameters of our input\nbatch_size = 32\nmax_sequence_len = 1024\nnum_heads = 32\nembed_dimension = 32\n\ndtype = torch.float16\n\nquery = torch.rand(batch_size, num_heads, max_sequence_len, embed_dimension, device=device, dtype=dtype)\nkey = torch.rand(batch_size, num_heads, max_sequence_len, embed_dimension, device=device, dtype=dtype)\nvalue = torch.rand(batch_size, num_heads, max_sequence_len, embed_dimension, device=device, dtype=dtype)\n\nprint(f\"The default implementation runs in {benchmark_torch_function_in_microseconds(F.scaled_dot_product_attention, query, key, value):.3f} microseconds\")\n\n# Lets explore the speed of each of the 3 implementations\nfrom torch.backends.cuda import sdp_kernel, SDPBackend\n\n# Helpful arguments mapper\nbackend_map = {\n    SDPBackend.MATH: {\"enable_math\": True, \"enable_flash\": False, \"enable_mem_efficient\": False},\n    SDPBackend.FLASH_ATTENTION: {\"enable_math\": False, \"enable_flash\": True, \"enable_mem_efficient\": False},\n    SDPBackend.EFFICIENT_ATTENTION: {\n        \"enable_math\": False, \"enable_flash\": False, \"enable_mem_efficient\": True}\n}\n\nwith sdp_kernel(**backend_map[SDPBackend.MATH]):\n    print(f\"The math implementation runs in {benchmark_torch_function_in_microseconds(F.scaled_dot_product_attention, query, key, value):.3f} microseconds\")\n\n\nwith sdp_kernel(**backend_map[SDPBackend.FLASH_ATTENTION]):\n    try:\n        print(f\"The flash attention implementation runs in {benchmark_torch_function_in_microseconds(F.scaled_dot_product_attention, query, key, value):.3f} microseconds\")\n    except RuntimeError:\n        print(\"FlashAttention is not supported. See warnings for reasons.\")\n\nwith sdp_kernel(**backend_map[SDPBackend.EFFICIENT_ATTENTION]):\n    try:\n        print(f\"The memory efficient implementation runs in {benchmark_torch_function_in_microseconds(F.scaled_dot_product_attention, query, key, value):.3f} microseconds\")\n    except RuntimeError:\n        print(\"EfficientAttention is not supported. See warnings for reasons.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hardware dependence\n\nDepending on what machine you ran the above cell on and what hardware is\navailable, your results might be different.\n- If you don\u2019t have a GPU and are running on CPU then the context manager\nwill have no effect and all three runs should return similar timings.\n- Depending on what compute capability your graphics card supports\nflash attention or memory efficient might have failed.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Causal Self Attention\n\nBelow is an example implementation of a multi-headed causal self\nattention block inspired by\n[Andrej Karpathy NanoGPT](https://github.com/karpathy/nanoGPT)_ repository.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class CausalSelfAttention(nn.Module):\n\n    def __init__(self, num_heads: int, embed_dimension: int, bias: bool=False, is_causal: bool=False, dropout:float=0.0):\n        super().__init__()\n        assert embed_dimension % num_heads == 0\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = nn.Linear(embed_dimension, 3 * embed_dimension, bias=bias)\n        # output projection\n        self.c_proj = nn.Linear(embed_dimension, embed_dimension, bias=bias)\n        # regularization\n        self.dropout = dropout\n        self.resid_dropout = nn.Dropout(dropout)\n        self.num_heads = num_heads\n        self.embed_dimension = embed_dimension\n        # Perform causal masking\n        self.is_causal = is_causal\n\n    def forward(self, x):\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        query_projected = self.c_attn(x)\n\n        batch_size = query_projected.size(0)\n        embed_dim = query_projected.size(2)\n        head_dim = embed_dim // (self.num_heads * 3)\n\n        query, key, value = query_projected.chunk(3, -1)\n        query = query.view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n        key = key.view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n        value = value.view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n\n        if self.training:\n            dropout = self.dropout\n            is_causal = self.is_causal\n        else:\n            dropout = 0.0\n            is_causal = False\n\n        y = F.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=dropout, is_causal=is_causal)\n        y = y.transpose(1, 2).view(batch_size, -1, self.num_heads * head_dim)\n\n        y = self.resid_dropout(self.c_proj(y))\n        return y\n\n\nnum_heads = 8\nheads_per_dim = 64\nembed_dimension = num_heads * heads_per_dim\ndtype = torch.float16\nmodel = CausalSelfAttention(num_heads=num_heads, embed_dimension=embed_dimension, bias=False, is_causal=True, dropout=0.1).to(\"cuda\").to(dtype).eval()\nprint(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ``NestedTensor`` and Dense tensor support\n\nSDPA supports both ``NestedTensor`` and Dense tensor inputs. ``NestedTensors`` handle the case where the input is a batch of variable length sequences\nwithout needing to pad each sequence to the maximum length in the batch. For more information about ``NestedTensors`` see\n[torch.nested](https://pytorch.org/docs/stable/nested.html)_ and [NestedTensors Tutorial](https://pytorch.org/tutorials/prototype/nestedtensor.html)_.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import random\ndef generate_rand_batch(\n    batch_size,\n    max_sequence_len,\n    embed_dimension,\n    pad_percentage=None,\n    dtype=torch.float16,\n    device=\"cuda\",\n):\n    if not pad_percentage:\n        return (\n            torch.randn(\n                batch_size,\n                max_sequence_len,\n                embed_dimension,\n                dtype=dtype,\n                device=device,\n            ),\n            None,\n        )\n    # Random sequence lengths\n    seq_len_list = [\n        int(max_sequence_len * (1 - random.gauss(pad_percentage, 0.01)))\n        for _ in range(batch_size)\n    ]\n    # Make random entry in the batch have max sequence length\n    seq_len_list[random.randint(0, batch_size - 1)] = max_sequence_len\n    return (\n        torch.nested.nested_tensor(\n            [\n                torch.randn(seq_len, embed_dimension,\n                            dtype=dtype, device=device)\n                for seq_len in seq_len_list\n            ]\n        ),\n        seq_len_list,\n    )\n\nrandom_nt, _ = generate_rand_batch(32, 512, embed_dimension, pad_percentage=0.5, dtype=dtype, device=device)\nrandom_dense, _ = generate_rand_batch(32, 512, embed_dimension, pad_percentage=None, dtype=dtype, device=device)\n\n# Currently the fused implementations don't support ``NestedTensor`` for training\nmodel.eval()\n\nwith sdp_kernel(**backend_map[SDPBackend.FLASH_ATTENTION]):\n    try:\n        print(f\"Random NT runs in {benchmark_torch_function_in_microseconds(model, random_nt):.3f} microseconds\")\n        print(f\"Random Dense runs in {benchmark_torch_function_in_microseconds(model, random_dense):.3f} microseconds\")\n    except RuntimeError:\n        print(\"FlashAttention is not supported. See warnings for reasons.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Using SDPA with ``torch.compile``\n\nWith the release of PyTorch 2.0, a new feature called\n``torch.compile()`` has been introduced, which can provide\nsignificant performance improvements over eager mode.\nScaled dot product attention is fully composable with ``torch.compile()``.\nTo demonstrate this, let's compile the ``CausalSelfAttention`` module using\n``torch.compile()`` and observe the resulting performance improvements.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batch_size = 32\nmax_sequence_len = 256\nx = torch.rand(batch_size, max_sequence_len,\n               embed_dimension, device=device, dtype=dtype)\nprint(\n    f\"The non compiled module runs in  {benchmark_torch_function_in_microseconds(model, x):.3f} microseconds\")\n\n\ncompiled_model = torch.compile(model)\n# Let's compile it\ncompiled_model(x)\nprint(\n    f\"The compiled module runs in  {benchmark_torch_function_in_microseconds(compiled_model, x):.3f} microseconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The exact execution time is dependent on machine, however the results for mine:\nThe non compiled module runs in  166.616 microseconds\nThe compiled module runs in  166.726 microseconds\nThat is not what we were expecting. Let's dig a little deeper.\nPyTorch comes with an amazing built-in profiler that you can use to\ninspect the performance characteristics of your code.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch.profiler import profile, record_function, ProfilerActivity\nactivities = [ProfilerActivity.CPU]\nif device == 'cuda':\n    activities.append(ProfilerActivity.CUDA)\n\nwith profile(activities=activities, record_shapes=False) as prof:\n    with record_function(\" Non-Compilied Causal Attention\"):\n        for _ in range(25):\n            model(x)\nprint(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n\n\nwith profile(activities=activities, record_shapes=False) as prof:\n    with record_function(\"Compiled Causal Attention\"):\n        for _ in range(25):\n            compiled_model(x)\nprint(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n\n# For even more insights, you can export the trace and use ``chrome://tracing`` to view the results\n#\n# .. code-block:: python\n#\n#    prof.export_chrome_trace(\"compiled_causal_attention_trace.json\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The previous code snippet generates a report of the top 10 PyTorch functions\nthat consumed the most GPU execution time, for both the compiled and non-compiled module.\nThe analysis reveals that the majority of time spent on the GPU is concentrated\non the same set of functions for both modules.\nThe reason for this here is that ``torch.compile`` is very good at removing the\nframework overhead associated with PyTorch. If your model is launching\nlarge, efficient CUDA kernels, which in this case ``CausalSelfAttention``\nis, then the overhead of PyTorch can be hidden.\n\nIn reality, your module does not normally consist of a singular\n``CausalSelfAttention`` block. When experimenting with [Andrej Karpathy NanoGPT](https://github.com/karpathy/nanoGPT)_ repository, compiling\nthe module took the time per train step from: ``6090.49ms`` to\n``3273.17ms``! This was done on commit: ``ae3a8d5`` of NanoGPT training on\nthe Shakespeare dataset.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusion\n\nIn this tutorial, we have demonstrated the basic usage of\n``torch.nn.functional.scaled_dot_product_attention``. We have shown how\nthe ``sdp_kernel`` context manager can be used to assert a certain\nimplementation is used on GPU. As well, we built a simple\n``CausalSelfAttention`` module that works with ``NestedTensor`` and is torch\ncompilable. In the process we have shown how to the profiling tools can\nbe used to explore the performance characteristics of a user defined\nmodule.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}